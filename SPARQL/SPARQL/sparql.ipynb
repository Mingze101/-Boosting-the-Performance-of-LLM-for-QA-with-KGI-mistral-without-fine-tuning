{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one given tail entity and one given relation - search head entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parse Turtle File\n",
    "Create a function or a standalone code block to load the Turtle file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '2x90 min/week'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '1,5x90 min/week'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6 weeks'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '3 hrs plus exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: 'Exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: 'Exercises'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '5/9/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '5/9/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '6/16/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '7/15/2021'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '4/20/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 148, in parse_time\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 time format: %r' % timestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 time format: '4/20/2023'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#time, Converter=<function parse_time at 0x000001D1235184C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"d:\\Anaconda\\envs\\iseenv\\lib\\site-packages\\isodate\\isotime.py\", line 146, in parse_time\n",
      "    return time(int(hour), int(minute), int(second),\n",
      "ValueError: hour must be in 0..23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turtle file loaded successfully.\n",
      "Number of triples in the graph: 8166\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "# Specify the path to your Turtle file\n",
    "turtle_file = '../Material/output.ttl'\n",
    "\n",
    "# Create a new RDFLib Graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Attempt to load the Turtle file into the graph\n",
    "try:\n",
    "    g.parse(turtle_file, format=\"turtle\")\n",
    "    print(\"Turtle file loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Turtle file: {e}\")\n",
    "        \n",
    "# Print the number of triples in the graph\n",
    "print(f\"Number of triples in the graph: {len(g)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Custom Function search_head_entities\n",
    "Modify the custom function to accept an RDFLib Graph object as a parameter instead of loading the Turtle file within the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `g.bind()` method is used in RDFLib to globally bind namespaces to a graph object, facilitating the manipulation of graph data. On the other hand, defining prefixes within SPARQL query strings serves to make individual queries self-contained, thereby enhancing readability and shareability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_head_entities(tail_entity_uri,relation_uri):\n",
    "#     # Prepare the SPARQL query with dynamic relation and tail entity URIs\n",
    "#     query = f\"\"\"\n",
    "#     PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "#     PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "#     PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "#     PREFIX emmo: <http://emmo.info/emmo#>\n",
    "#     PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "#     PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "#     PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "#     PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "#     PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "#     PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "#     PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "#     PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "#     PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "#     PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "#     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "#     PREFIX schema: <https://schema.org/>\n",
    "#     PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "#     PREFIX void: <http://rdfs.org/ns/void#>\n",
    "#     PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "#     SELECT ?headEntity\n",
    "#     WHERE {{\n",
    "#       ?headEntity {relation_uri} <{tail_entity_uri}> .\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Execute the query and collect results\n",
    "#     triples = []\n",
    "#     try:\n",
    "#         results = g.query(query)\n",
    "#         for row in results:\n",
    "#             # Format each result as a string without angle brackets\n",
    "#             head_entity = str(row.headEntity)\n",
    "#             if head_entity.startswith(\"http\"):\n",
    "#                 head_entity = head_entity  # URIs are output directly without <>\n",
    "#             else:\n",
    "#                 head_entity = f\":{head_entity}\"  # Prefixed names are prefixed with a colon\n",
    "\n",
    "#             # Assuming relation_uri is a prefixed name, not a full URI, so it's output directly\n",
    "#             tail_entity = tail_entity_uri  # URIs are output directly without <>\n",
    "\n",
    "#             triple = f\"{head_entity} {relation_uri} {tail_entity} .\"\n",
    "#             triples.append(triple)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SPARQL query: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_head_entities(tail_entity_uri, relation_uri):\n",
    "    # Prepare the SPARQL query with dynamic relation and tail entity URIs\n",
    "    query = f\"\"\"\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "    PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX emmo: <http://emmo.info/emmo#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "    PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "    PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "    PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "    PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX schema: <https://schema.org/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX void: <http://rdfs.org/ns/void#>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "    SELECT ?headEntity\n",
    "    WHERE {{\n",
    "      ?headEntity {relation_uri} <{tail_entity_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and collect results as tuples\n",
    "    triples = []\n",
    "    try:\n",
    "        results = g.query(query)\n",
    "        for row in results:\n",
    "            head_entity = str(row.headEntity)\n",
    "            if head_entity.startswith(\"http\"):\n",
    "                head_entity = head_entity  # URIs are output directly without <>\n",
    "            else:\n",
    "                head_entity = f\":{head_entity}\"  # Prefixed names are prefixed with a colon\n",
    "\n",
    "            # Assuming relation_uri is a prefixed name, not a full URI, so it's output directly\n",
    "            tail_entity = tail_entity_uri  # URIs are output directly without <>\n",
    "\n",
    "            triple = f\"('{head_entity}', '{relation_uri}', '{tail_entity}')\"\n",
    "            triples.append(triple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Functions search_head_entities\n",
    "First, load the Turtle file and bind the namespaces, then pass the resulting Graph object to the custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1012754', 'mwo:hasWebsite', 'http://demo.fiz-karlsruhe.de/matwerk/E340077')\n"
     ]
    }
   ],
   "source": [
    "relation_uri = 'mwo:hasWebsite'\n",
    "tail_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E340077'\n",
    "triples = search_head_entities(tail_entity_uri,relation_uri)\n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one given head entity and one given relation - search tail entity, include literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Custom Function search_tail_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rdflib\n",
    "\n",
    "# def search_tail_entities(head_entity_uri, relation_uri):\n",
    "#     # Assuming 'g' is your RDFLib Graph object loaded with your Turtle data\n",
    "#     global g\n",
    "\n",
    "#     # Prepare the SPARQL query with dynamic head entity and relation URIs\n",
    "#     query = f\"\"\"\n",
    "#     PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "#     PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "#     PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "#     PREFIX emmo: <http://emmo.info/emmo#>\n",
    "#     PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "#     PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "#     PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "#     PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "#     PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "#     PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "#     PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "#     PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "#     PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "#     PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "#     PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "#     PREFIX schema: <https://schema.org/>\n",
    "#     PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "#     PREFIX void: <http://rdfs.org/ns/void#>\n",
    "#     PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "#     SELECT ?tailEntity\n",
    "#     WHERE {{\n",
    "#       <{head_entity_uri}> {relation_uri} ?tailEntity .\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Execute the query and collect results\n",
    "#     turtle_triples = []\n",
    "#     try:\n",
    "#         results = g.query(query)\n",
    "#         for row in results:\n",
    "#             # Construct Turtle triple string without angle brackets for the head entity URI\n",
    "#             tail_entity = row.tailEntity.toPython() if isinstance(row.tailEntity, rdflib.URIRef) else f\"\\\"{row.tailEntity}\\\"\"\n",
    "#             triple = f\"{head_entity_uri} {relation_uri} {tail_entity} .\"\n",
    "#             turtle_triples.append(triple)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SPARQL query: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     return turtle_triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "def search_tail_entities(head_entity_uri, relation_uri):\n",
    "    # Assuming 'g' is your RDFLib Graph object loaded with your Turtle data\n",
    "    global g\n",
    "\n",
    "    # Prepare the SPARQL query with dynamic head entity and relation URIs\n",
    "    query = f\"\"\"\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "    PREFIX default1: <https://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX emmo: <http://emmo.info/emmo#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX modsci: <https://w3id.org/skgo/modsci#>\n",
    "    PREFIX mwo: <http://purls.helmholtz-metadaten.de/mwo/>\n",
    "    PREFIX nfdicore: <http://nfdi.fiz-karlsruhe.de/ontology/>\n",
    "    PREFIX ns1: <https://w3id.org/scholarlydata/ontology/conference-ontology.owl#>\n",
    "    PREFIX ns2: <http://purl.obolibrary.org/obo/>\n",
    "    PREFIX ns3: <http://www.ebi.ac.uk/swo/>\n",
    "    PREFIX ns4: <http://www.geneontology.org/formats/oboInOwl#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX schema: <https://schema.org/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX void: <http://rdfs.org/ns/void#>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "    SELECT ?tailEntity\n",
    "    WHERE {{\n",
    "      <{head_entity_uri}> {relation_uri} ?tailEntity .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and collect results as tuples\n",
    "    triples = []\n",
    "    try:\n",
    "        results = g.query(query)\n",
    "        for row in results:\n",
    "            # Construct a tuple for each triple\n",
    "            tail_entity = row.tailEntity.toPython() if isinstance(row.tailEntity, rdflib.URIRef) else f\"\\\"{row.tailEntity}\\\"\"\n",
    "            triple = (head_entity_uri, relation_uri, tail_entity)\n",
    "            triples.append(triple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Custom Function search_tail_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1004237', 'mwo:description', '\"GitHub, Inc., is an Internet hosting service for software development and version control using Git. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project.\"')\n"
     ]
    }
   ],
   "source": [
    "head_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E1004237'\n",
    "relation_uri = 'mwo:description'\n",
    "\n",
    "turtle_triples = search_tail_entities(head_entity_uri, relation_uri)\n",
    "for triple in turtle_triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E15879', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n"
     ]
    }
   ],
   "source": [
    "head_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E15879'\n",
    "relation_uri = 'mwo:hasAffiliation'\n",
    "\n",
    "turtle_triples = search_tail_entities(head_entity_uri, relation_uri)\n",
    "for triple in turtle_triples:\n",
    "    print(triple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one given head entity and one given relation, firstly find tail entity, then find all head entities with same tail entity and same relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Custom Function search_related_head_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "def search_related_head_entities(initial_head_entity_uri, relation_uri):\n",
    "    # Define all the prefix namespaces\n",
    "    mwo_prefix = \"http://purls.helmholtz-metadaten.de/mwo/\"\n",
    "    dc_prefix = \"http://purl.org/dc/elements/1.1/\"\n",
    "    dcterms_prefix = \"http://purl.org/dc/terms/\"\n",
    "    default1_prefix = \"https://nfdi.fiz-karlsruhe.de/ontology/\"\n",
    "    emmo_prefix = \"http://emmo.info/emmo#\"\n",
    "    foaf_prefix = \"http://xmlns.com/foaf/0.1/\"\n",
    "    modsci_prefix = \"https://w3id.org/skgo/modsci#\"\n",
    "    nfdicore_prefix = \"http://nfdi.fiz-karlsruhe.de/ontology/\"\n",
    "    ns1_prefix = \"https://w3id.org/scholarlydata/ontology/conference-ontology.owl#\"\n",
    "    ns2_prefix = \"http://purl.obolibrary.org/obo/\"\n",
    "    ns3_prefix = \"http://www.ebi.ac.uk/swo/\"\n",
    "    ns4_prefix = \"http://www.geneontology.org/formats/oboInOwl#\"\n",
    "    owl_prefix = \"http://www.w3.org/2002/07/owl#\"\n",
    "    rdf_prefix = \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
    "    rdfs_prefix = \"http://www.w3.org/2000/01/rdf-schema#\"\n",
    "    schema_prefix = \"https://schema.org/\"\n",
    "    skos_prefix = \"http://www.w3.org/2004/02/skos/core#\"\n",
    "    void_prefix = \"http://rdfs.org/ns/void#\"\n",
    "    xsd_prefix = \"http://www.w3.org/2001/XMLSchema#\"\n",
    "\n",
    "    # Step 1: Find the tail entity associated with the given head entity and relation\n",
    "    first_query = f\"\"\"\n",
    "    PREFIX mwo: <{mwo_prefix}>\n",
    "    PREFIX dc: <{dc_prefix}>\n",
    "    PREFIX dcterms: <{dcterms_prefix}>\n",
    "    PREFIX default1: <{default1_prefix}>\n",
    "    PREFIX emmo: <{emmo_prefix}>\n",
    "    PREFIX foaf: <{foaf_prefix}>\n",
    "    PREFIX modsci: <{modsci_prefix}>\n",
    "    PREFIX nfdicore: <{nfdicore_prefix}>\n",
    "    PREFIX ns1: <{ns1_prefix}>\n",
    "    PREFIX ns2: <{ns2_prefix}>\n",
    "    PREFIX ns3: <{ns3_prefix}>\n",
    "    PREFIX ns4: <{ns4_prefix}>\n",
    "    PREFIX owl: <{owl_prefix}>\n",
    "    PREFIX rdf: <{rdf_prefix}>\n",
    "    PREFIX rdfs: <{rdfs_prefix}>\n",
    "    PREFIX schema: <{schema_prefix}>\n",
    "    PREFIX skos: <{skos_prefix}>\n",
    "    PREFIX void: <{void_prefix}>\n",
    "    PREFIX xsd: <{xsd_prefix}>\n",
    "    \n",
    "    SELECT ?tailEntity\n",
    "    WHERE {{\n",
    "        <{initial_head_entity_uri}> {relation_uri} ?tailEntity .\n",
    "    }}\n",
    "    LIMIT 1  # Assuming each head entity is associated with a unique tail entity\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        first_result = g.query(first_query)\n",
    "        tail_entity_list = [row for row in first_result]\n",
    "        if not tail_entity_list:\n",
    "            # print(\"No tail entity found for the given head entity and relation.\")\n",
    "            return []\n",
    "        tail_entity = tail_entity_list[0].tailEntity\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing the first SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Step 2: Find all other head entities that have the same relation with the found tail entity\n",
    "    second_query = f\"\"\"\n",
    "    PREFIX mwo: <{mwo_prefix}>\n",
    "    PREFIX dc: <{dc_prefix}>\n",
    "    PREFIX dcterms: <{dcterms_prefix}>\n",
    "    PREFIX default1: <{default1_prefix}>\n",
    "    PREFIX emmo: <{emmo_prefix}>\n",
    "    PREFIX foaf: <{foaf_prefix}>\n",
    "    PREFIX modsci: <{modsci_prefix}>\n",
    "    PREFIX nfdicore: <{nfdicore_prefix}>\n",
    "    PREFIX ns1: <{ns1_prefix}>\n",
    "    PREFIX ns2: <{ns2_prefix}>\n",
    "    PREFIX ns3: <{ns3_prefix}>\n",
    "    PREFIX ns4: <{ns4_prefix}>\n",
    "    PREFIX owl: <{owl_prefix}>\n",
    "    PREFIX rdf: <{rdf_prefix}>\n",
    "    PREFIX rdfs: <{rdfs_prefix}>\n",
    "    PREFIX schema: <{schema_prefix}>\n",
    "    PREFIX skos: <{skos_prefix}>\n",
    "    PREFIX void: <{void_prefix}>\n",
    "    PREFIX xsd: <{xsd_prefix}>\n",
    "    \n",
    "    SELECT ?otherHeadEntity\n",
    "    WHERE {{\n",
    "        ?otherHeadEntity {relation_uri} <{tail_entity}> .\n",
    "        FILTER (?otherHeadEntity != <{initial_head_entity_uri}>)  # Exclude the initial head entity\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        second_results = g.query(second_query)\n",
    "        triples = [(str(row.otherHeadEntity), relation_uri, str(tail_entity)) for row in second_results]\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing the second SPARQL query: {e}\")\n",
    "        return []\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Custom Function search_related_head_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://demo.fiz-karlsruhe.de/matwerk/E1245566', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E16052', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E9779', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E14862', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n"
     ]
    }
   ],
   "source": [
    "initial_head_entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E15879'\n",
    "relation_uri = 'mwo:hasAffiliation'\n",
    "\n",
    "related_head_entities = search_related_head_entities(initial_head_entity_uri, relation_uri)\n",
    "for entity in related_head_entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for all relevant triples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_possible_triples(entity_uri, relation_uri):\n",
    "    # Call the custom functions to find relevant triples\n",
    "    triples_query1 = search_head_entities(entity_uri, relation_uri)\n",
    "    triples_query2 = search_tail_entities(entity_uri, relation_uri)\n",
    "    triples_query3 = search_related_head_entities(entity_uri, relation_uri)\n",
    "    \n",
    "    # Combine the results from all three queries\n",
    "    all_triples = triples_query1 + triples_query2 + triples_query3\n",
    "    \n",
    "    return all_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Triples:\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E15879', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E1245566', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E16052', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E9779', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E14862', 'mwo:hasAffiliation', 'http://demo.fiz-karlsruhe.de/matwerk/E1016')\n"
     ]
    }
   ],
   "source": [
    "entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E15879'\n",
    "relation_uri = 'mwo:hasAffiliation'\n",
    "\n",
    "# Call the search_all_possible_triples function\n",
    "triples = search_all_possible_triples(entity_uri, relation_uri)\n",
    "\n",
    "\n",
    "# Verify the returned triples\n",
    "print(\"Found Triples:\")\n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Triples:\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E10317', 'mwo:hasExpertiseIn', 'http://demo.fiz-karlsruhe.de/matwerk/E67431')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E8627', 'mwo:hasExpertiseIn', 'http://demo.fiz-karlsruhe.de/matwerk/E67431')\n"
     ]
    }
   ],
   "source": [
    "entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E67431'\n",
    "relation_uri = 'mwo:hasExpertiseIn'\n",
    "\n",
    "# Call the search_all_possible_triples function\n",
    "triples = search_all_possible_triples(entity_uri, relation_uri)\n",
    "\n",
    "# Verify the returned triples\n",
    "print(\"Found Triples:\")\n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Triples:\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E12326', 'mwo:hasExpertiseIn', 'http://demo.fiz-karlsruhe.de/matwerk/E49517')\n",
      "('http://demo.fiz-karlsruhe.de/matwerk/E12629', 'mwo:hasExpertiseIn', 'http://demo.fiz-karlsruhe.de/matwerk/E49517')\n"
     ]
    }
   ],
   "source": [
    "entity_uri = 'http://demo.fiz-karlsruhe.de/matwerk/E49517'\n",
    "relation_uri =  'mwo:hasExpertiseIn'\n",
    "\n",
    "# Call the search_all_possible_triples function\n",
    "triples = search_all_possible_triples(entity_uri, relation_uri)\n",
    "\n",
    "# Verify the returned triples\n",
    "print(\"Found Triples:\")\n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file from Dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Predicate Verbs</th>\n",
       "      <th>Similar Relations</th>\n",
       "      <th>Relation URIs</th>\n",
       "      <th>Relation_uris_with_Namespace</th>\n",
       "      <th>Similar Entities</th>\n",
       "      <th>Entity URIs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is working in the Computational Materials ...</td>\n",
       "      <td>the Computational Materials Science</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasW...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Computational Materials Science', 'Computati...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E42042'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the research projects associated to E...</td>\n",
       "      <td>EMMO</td>\n",
       "      <td>research project associate</td>\n",
       "      <td>['has related Project', 'related participant p...</td>\n",
       "      <td>['http://nfdi.fiz-karlsruhe.de/ontology/relate...</td>\n",
       "      <td>['nfdicore:relatedProject', 'mwo:relatedPartic...</td>\n",
       "      <td>['R. S. Elliott and E. B. Tadmor, \"Knowledgeba...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E115240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the contributors of the data \"datasets\"?</td>\n",
       "      <td>datasets</td>\n",
       "      <td>contributor</td>\n",
       "      <td>['has contributor', 'related participant proje...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasC...</td>\n",
       "      <td>['mwo:hasContributor', 'mwo:relatedParticipant...</td>\n",
       "      <td>['Framework for curation and distribution of r...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E124838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is working with Researcher \"Ebrahim Norouz...</td>\n",
       "      <td>Ebrahim Norouzi</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasW...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Ebrahim Norouzi', 'Ahmad Zainul Ihsan', 'Hos...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E124556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the email address of \"ParaView\"?</td>\n",
       "      <td>ParaView</td>\n",
       "      <td>email address</td>\n",
       "      <td>['has email address ', 'has postal address', '...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/emai...</td>\n",
       "      <td>['mwo:emailAddress', 'mwo:hasPostalAddress', '...</td>\n",
       "      <td>['data portal', 'ParaView', 'paraview', 'datas...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E419156...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Who is working in the Computational Materials ...   \n",
       "1  What are the research projects associated to E...   \n",
       "2   Who are the contributors of the data \"datasets\"?   \n",
       "3  Who is working with Researcher \"Ebrahim Norouz...   \n",
       "4            Who is the email address of \"ParaView\"?   \n",
       "\n",
       "                        Named Entities              Predicate Verbs  \\\n",
       "0  the Computational Materials Science                         work   \n",
       "1                                 EMMO   research project associate   \n",
       "2                             datasets                  contributor   \n",
       "3                      Ebrahim Norouzi                         work   \n",
       "4                             ParaView                email address   \n",
       "\n",
       "                                   Similar Relations  \\\n",
       "0  ['has work package', 'has expertise in', 'has ...   \n",
       "1  ['has related Project', 'related participant p...   \n",
       "2  ['has contributor', 'related participant proje...   \n",
       "3  ['has work package', 'has expertise in', 'has ...   \n",
       "4  ['has email address ', 'has postal address', '...   \n",
       "\n",
       "                                       Relation URIs  \\\n",
       "0  ['http://purls.helmholtz-metadaten.de/mwo/hasW...   \n",
       "1  ['http://nfdi.fiz-karlsruhe.de/ontology/relate...   \n",
       "2  ['http://purls.helmholtz-metadaten.de/mwo/hasC...   \n",
       "3  ['http://purls.helmholtz-metadaten.de/mwo/hasW...   \n",
       "4  ['http://purls.helmholtz-metadaten.de/mwo/emai...   \n",
       "\n",
       "                        Relation_uris_with_Namespace  \\\n",
       "0  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "1  ['nfdicore:relatedProject', 'mwo:relatedPartic...   \n",
       "2  ['mwo:hasContributor', 'mwo:relatedParticipant...   \n",
       "3  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "4  ['mwo:emailAddress', 'mwo:hasPostalAddress', '...   \n",
       "\n",
       "                                    Similar Entities  \\\n",
       "0  ['Computational Materials Science', 'Computati...   \n",
       "1  ['R. S. Elliott and E. B. Tadmor, \"Knowledgeba...   \n",
       "2  ['Framework for curation and distribution of r...   \n",
       "3  ['Ebrahim Norouzi', 'Ahmad Zainul Ihsan', 'Hos...   \n",
       "4  ['data portal', 'ParaView', 'paraview', 'datas...   \n",
       "\n",
       "                                         Entity URIs  \n",
       "0  ['http://demo.fiz-karlsruhe.de/matwerk/E42042'...  \n",
       "1  ['http://demo.fiz-karlsruhe.de/matwerk/E115240...  \n",
       "2  ['http://demo.fiz-karlsruhe.de/matwerk/E124838...  \n",
       "3  ['http://demo.fiz-karlsruhe.de/matwerk/E124556...  \n",
       "4  ['http://demo.fiz-karlsruhe.de/matwerk/E419156...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../Similarity_Matching/relevant_entities_relations.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n",
      "Error executing the second SPARQL query: Expected SelectQuery, found '?'  (at char 1077), (line:24, col:9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Predicate Verbs</th>\n",
       "      <th>Similar Relations</th>\n",
       "      <th>Relation URIs</th>\n",
       "      <th>Relation_uris_with_Namespace</th>\n",
       "      <th>Similar Entities</th>\n",
       "      <th>Entity URIs</th>\n",
       "      <th>Found Triples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is working in the Computational Materials ...</td>\n",
       "      <td>the Computational Materials Science</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasW...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Computational Materials Science', 'Computati...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E42042'...</td>\n",
       "      <td>[\"('http://demo.fiz-karlsruhe.de/matwerk/E1115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the research projects associated to E...</td>\n",
       "      <td>EMMO</td>\n",
       "      <td>research project associate</td>\n",
       "      <td>['has related Project', 'related participant p...</td>\n",
       "      <td>['http://nfdi.fiz-karlsruhe.de/ontology/relate...</td>\n",
       "      <td>['nfdicore:relatedProject', 'mwo:relatedPartic...</td>\n",
       "      <td>['R. S. Elliott and E. B. Tadmor, \"Knowledgeba...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E115240...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E11524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the contributors of the data \"datasets\"?</td>\n",
       "      <td>datasets</td>\n",
       "      <td>contributor</td>\n",
       "      <td>['has contributor', 'related participant proje...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasC...</td>\n",
       "      <td>['mwo:hasContributor', 'mwo:relatedParticipant...</td>\n",
       "      <td>['Framework for curation and distribution of r...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E124838...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E12483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is working with Researcher \"Ebrahim Norouz...</td>\n",
       "      <td>Ebrahim Norouzi</td>\n",
       "      <td>work</td>\n",
       "      <td>['has work package', 'has expertise in', 'has ...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/hasW...</td>\n",
       "      <td>['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...</td>\n",
       "      <td>['Ebrahim Norouzi', 'Ahmad Zainul Ihsan', 'Hos...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E124556...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E12455...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the email address of \"ParaView\"?</td>\n",
       "      <td>ParaView</td>\n",
       "      <td>email address</td>\n",
       "      <td>['has email address ', 'has postal address', '...</td>\n",
       "      <td>['http://purls.helmholtz-metadaten.de/mwo/emai...</td>\n",
       "      <td>['mwo:emailAddress', 'mwo:hasPostalAddress', '...</td>\n",
       "      <td>['data portal', 'ParaView', 'paraview', 'datas...</td>\n",
       "      <td>['http://demo.fiz-karlsruhe.de/matwerk/E419156...</td>\n",
       "      <td>[('http://demo.fiz-karlsruhe.de/matwerk/E41915...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Who is working in the Computational Materials ...   \n",
       "1  What are the research projects associated to E...   \n",
       "2   Who are the contributors of the data \"datasets\"?   \n",
       "3  Who is working with Researcher \"Ebrahim Norouz...   \n",
       "4            Who is the email address of \"ParaView\"?   \n",
       "\n",
       "                        Named Entities              Predicate Verbs  \\\n",
       "0  the Computational Materials Science                         work   \n",
       "1                                 EMMO   research project associate   \n",
       "2                             datasets                  contributor   \n",
       "3                      Ebrahim Norouzi                         work   \n",
       "4                             ParaView                email address   \n",
       "\n",
       "                                   Similar Relations  \\\n",
       "0  ['has work package', 'has expertise in', 'has ...   \n",
       "1  ['has related Project', 'related participant p...   \n",
       "2  ['has contributor', 'related participant proje...   \n",
       "3  ['has work package', 'has expertise in', 'has ...   \n",
       "4  ['has email address ', 'has postal address', '...   \n",
       "\n",
       "                                       Relation URIs  \\\n",
       "0  ['http://purls.helmholtz-metadaten.de/mwo/hasW...   \n",
       "1  ['http://nfdi.fiz-karlsruhe.de/ontology/relate...   \n",
       "2  ['http://purls.helmholtz-metadaten.de/mwo/hasC...   \n",
       "3  ['http://purls.helmholtz-metadaten.de/mwo/hasW...   \n",
       "4  ['http://purls.helmholtz-metadaten.de/mwo/emai...   \n",
       "\n",
       "                        Relation_uris_with_Namespace  \\\n",
       "0  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "1  ['nfdicore:relatedProject', 'mwo:relatedPartic...   \n",
       "2  ['mwo:hasContributor', 'mwo:relatedParticipant...   \n",
       "3  ['mwo:hasWorkPackage', 'mwo:hasExpertiseIn', '...   \n",
       "4  ['mwo:emailAddress', 'mwo:hasPostalAddress', '...   \n",
       "\n",
       "                                    Similar Entities  \\\n",
       "0  ['Computational Materials Science', 'Computati...   \n",
       "1  ['R. S. Elliott and E. B. Tadmor, \"Knowledgeba...   \n",
       "2  ['Framework for curation and distribution of r...   \n",
       "3  ['Ebrahim Norouzi', 'Ahmad Zainul Ihsan', 'Hos...   \n",
       "4  ['data portal', 'ParaView', 'paraview', 'datas...   \n",
       "\n",
       "                                         Entity URIs  \\\n",
       "0  ['http://demo.fiz-karlsruhe.de/matwerk/E42042'...   \n",
       "1  ['http://demo.fiz-karlsruhe.de/matwerk/E115240...   \n",
       "2  ['http://demo.fiz-karlsruhe.de/matwerk/E124838...   \n",
       "3  ['http://demo.fiz-karlsruhe.de/matwerk/E124556...   \n",
       "4  ['http://demo.fiz-karlsruhe.de/matwerk/E419156...   \n",
       "\n",
       "                                       Found Triples  \n",
       "0  [\"('http://demo.fiz-karlsruhe.de/matwerk/E1115...  \n",
       "1  [('http://demo.fiz-karlsruhe.de/matwerk/E11524...  \n",
       "2  [('http://demo.fiz-karlsruhe.de/matwerk/E12483...  \n",
       "3  [('http://demo.fiz-karlsruhe.de/matwerk/E12455...  \n",
       "4  [('http://demo.fiz-karlsruhe.de/matwerk/E41915...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "# Initialize a new column in the DataFrame to store the found triples for each row\n",
    "df['Found Triples'] = None\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    found_triples = []  # Initialize an empty list to store the found triples for the current row\n",
    "\n",
    "    # Parse the 'Entity URIs' and 'relationship_uris_withNS' columns from the current row\n",
    "    entities_uris = ast.literal_eval(row['Entity URIs'])\n",
    "    relationship_uris = ast.literal_eval(row['Relation_uris_with_Namespace'])\n",
    "\n",
    "    # Iterate through each combination of entity URI and relationship URI\n",
    "    for entity_uri in entities_uris:\n",
    "        for relation_uri in relationship_uris:\n",
    "            triples = search_all_possible_triples(entity_uri, relation_uri)\n",
    "            for triple in triples:\n",
    "                if triple not in found_triples:  # Check if the triple is not already in the list\n",
    "                    found_triples.append(triple)\n",
    "    \n",
    "    # Store the found triples for the current row in the new column\n",
    "    df.at[index, 'Found Triples'] = str(found_triples)  # Convert the list to a string for storing in the DataFrame\n",
    "\n",
    "df.to_excel('sparql_output.xlsx', index=False)  # Save the updated DataFrame to a new Excel file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two coloumns\"Text\"\"Found Triples\" as new dataframe\n",
    "df_nwe= df[['Text', 'Found Triples']]\n",
    "df_nwe.to_excel('for_verbalisation.xlsx', index=False)  # Save the updated DataFrame to a new Excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iseenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
